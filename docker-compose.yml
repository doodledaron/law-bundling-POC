# NOTE: Flower has problem
# temporary fix, dont run flower first
# - docker-compose down
# - docker-compose up -d --build web redis worker

version: '3'

services:
  # FastAPI web application
  web:
    build: .
    ports:
      - "10000:10000"
    volumes:
      - .:/app
    depends_on:
      - redis
    environment:
      - REDIS_URL=redis://redis:6379/0
      - TZ=UTC
      # Memory optimization environment variables
      - OMP_NUM_THREADS=1
      - OPENBLAS_NUM_THREADS=1
      - MKL_NUM_THREADS=1
      - VECLIB_MAXIMUM_THREADS=1
      - NUMEXPR_NUM_THREADS=1
      # Python garbage collection settings
      - PYTHONGC=1
    # Disabled auto-reload to prevent high memory usage during directory scanning
    # Auto-reload constantly scans the entire directory tree for file changes
    # which can cause memory issues with large dependencies like sympy
    command: uvicorn main:app --host 0.0.0.0 --port 10000 --workers 1
    deploy:
      resources:
        limits:
          memory: 1.5G  # Limit memory to prevent container from using too much system memory
          cpus: '1'
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:10000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
      
  # Redis for message broker and result backend
  redis:
    image: redis:7-alpine  # Using alpine for smaller footprint
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    # Added memory optimization settings
    command: redis-server --save 60 1 --loglevel warning --maxmemory 80mb --maxmemory-policy allkeys-lru
    deploy:
      resources:
        limits:
          memory: 100M  # Limit memory to prevent container from using too much system memory
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Celery worker for processing tasks
  worker:
    build: .
    volumes:
      - .:/app
    depends_on:
      - redis
    environment:
      - REDIS_URL=redis://redis:6379/0
      - TZ=UTC
      # Memory optimization environment variables
      - OMP_NUM_THREADS=1
      - OPENBLAS_NUM_THREADS=1
      - MKL_NUM_THREADS=1
      - VECLIB_MAXIMUM_THREADS=1
      - NUMEXPR_NUM_THREADS=1
      # Python garbage collection tuning
      - PYTHONGC=1  
      # Control memory allocation behavior
      - MALLOC_TRIM_THRESHOLD_=65536   # More aggressive malloc trimming
      - MALLOC_ARENA_MAX=2             # Limit memory arenas
      - MALLOC_MMAP_THRESHOLD_=131072  # Lower threshold for mmap allocation
    # Set concurrency to 1 for very large documents
    # Reduce max tasks per child to recycle workers more frequently
    command: >
      celery -A tasks.celery_app worker 
      --loglevel=info 
      --max-tasks-per-child=5  
      --concurrency=2
      --without-gossip 
      --without-mingle
      --without-heartbeat
      --prefetch-multiplier=1
    deploy:
      resources:
        limits:
          memory: 6G  # Allocate more memory to the worker as it handles PDF processing
          cpus: '2'
    # Add optional restart policy to recover from OOM
    restart: on-failure:5
    # Add log rotation settings
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Flower for monitoring Celery tasks
  flower:
    build: .
    ports:
      - "5555:5555"
    depends_on:
      - redis
      - worker
    environment:
      - REDIS_URL=redis://redis:6379/0
    command: celery -A tasks.celery_app flower --port=5555
    deploy:
      resources:
        limits:
          memory: 1G

volumes:
  redis_data:

# 1. Memory Control
# With --prefetch-multiplier=1 and concurrency=2, your worker will only prefetch 2 tasks (1 per process) instead of 8:

# Before: Up to 8 documents could be loaded in memory at once
# After: Only 2 documents will be in memory at once (1 per worker process)