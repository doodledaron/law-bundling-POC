{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B8nnWIT7ROD"
      },
      "source": [
        "# LayoutLMv3 for Legal Document Analysis\n",
        "\n",
        "This notebook implements a streamlined pipeline for using LayoutLMv3 to analyze legal documents. The code leverages the existing Python scripts from the repository.\n",
        "\n",
        "1. **Setup Environment**: Configure the runtime environment (local or Colab)\n",
        "2. **Data Preparation**: If needed, prepare the CUAD dataset for LayoutLMv3\n",
        "3. **Model Training**: Fine-tune LayoutLMv3 for legal document understanding\n",
        "4. **Inference**: Apply the trained model to new documents\n",
        "\n",
        "This notebook assumes you already have the processor files set up in the `layoutlmv3/processor` directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL3eglB_7ROH"
      },
      "source": [
        "## 1. Setup Environment\n",
        "\n",
        "First, let's set up the environment. When running in Google Colab, we'll mount Google Drive to access and save files. Skip this if running locally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rinC8Ha7ROI"
      },
      "source": [
        "### 1.1 Check Environment and Setup Google Drive (for Colab)\n",
        "\n",
        "First, we'll check if we're in Colab and set up Google Drive access if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG0V4T-I7ROI",
        "outputId": "6227e6cc-f41f-43d3-9fbf-ac7bdfbcdc60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in local environment\n",
            "Working directory set to: C:\\freelance\\Leon\\law bundling POC\n"
          ]
        }
      ],
      "source": [
        "# Check if running in Colab\n",
        "import sys\n",
        "import os\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Running in Google Colab environment\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Set and change to base path\n",
        "    base_path = \"/content/drive/MyDrive/law bundling POC\"\n",
        "    os.chdir(base_path)\n",
        "\n",
        "    print(\"Working directory set to:\", os.getcwd())\n",
        "else:\n",
        "    print(\"Running in local environment\")\n",
        "    # Optional local setup\n",
        "    base_path = r\"C:\\freelance\\Leon\\law bundling POC\"  # Using raw string\n",
        "    # OR\n",
        "    # base_path = \"C:/freelance/Leon/law bundling POC/layoutlmv3\"  # Using forward slashes\n",
        "    # OR\n",
        "    # base_path = \"C:\\\\freelance\\\\Leon\\\\law bundling POC\\\\layoutlmv3\"  # Using escaped backslashes\n",
        "    os.chdir(base_path)\n",
        "    print(\"Working directory set to:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xP7NaKQS-qm",
        "outputId": "c07fb545-7927-4f77-f66a-88ce08e11d45"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'ls' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!ls CUAD_v1/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGzYaBem7ROK"
      },
      "source": [
        "### 1.2 Install Required Packages\n",
        "\n",
        "Now let's install all the dependencies needed for the project (if not already installed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y4JmHMy7ROK",
        "outputId": "ced5bd84-9a35-40bc-e5f8-3f41a5066417"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (2.2.0+cu118)\n",
            "Requirement already satisfied: transformers in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (4.48.3)\n",
            "Requirement already satisfied: pdf2image in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (1.17.0)\n",
            "Requirement already satisfied: pillow in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (11.1.0)\n",
            "Requirement already satisfied: matplotlib in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (3.10.0)\n",
            "Requirement already satisfied: opencv-python in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (4.11.0.86)\n",
            "Requirement already satisfied: tqdm in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: paddlepaddle in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (2.6.2)\n",
            "Requirement already satisfied: paddleocr in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (2.9.1)\n",
            "Requirement already satisfied: filelock in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from torch) (2025.2.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: colorama in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from scikit-learn) (1.15.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: httpx in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from paddlepaddle) (0.28.1)\n",
            "Requirement already satisfied: decorator in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from paddlepaddle) (5.1.1)\n",
            "Requirement already satisfied: astor in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from paddlepaddle) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum==3.3.0 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from paddlepaddle) (3.3.0)\n",
            "Requirement already satisfied: protobuf<=3.20.2,>=3.1.0 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from paddlepaddle) (3.20.2)\n",
            "Requirement already satisfied: shapely in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from paddleocr) (2.0.7)\n",
            "Requirement already satisfied: scikit-image in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from paddleocr) (0.25.1)\n",
            "Requirement already satisfied: imgaug in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from paddleocr) (0.4.0)\n",
            "Requirement already satisfied: pyclipper in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from paddleocr) (1.3.0.post6)\n",
            "Requirement already satisfied: lmdb in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from paddleocr) (1.6.2)\n",
            "Requirement already satisfied: rapidfuzz in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from paddleocr) (3.12.1)\n",
            "Requirement already satisfied: opencv-contrib-python in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from paddleocr) (4.11.0.86)\n",
            "Requirement already satisfied: cython in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from paddleocr) (3.0.12)\n",
            "Requirement already satisfied: python-docx in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from paddleocr) (1.1.2)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from paddleocr) (4.13.3)\n",
            "Requirement already satisfied: fire>=0.3.0 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from paddleocr) (0.7.0)\n",
            "Requirement already satisfied: albumentations==1.4.10 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from paddleocr) (1.4.10)\n",
            "Requirement already satisfied: albucore==0.0.13 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from paddleocr) (0.0.13)\n",
            "Requirement already satisfied: tomli>=2.0.1 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from albucore==0.0.13->paddleocr) (2.2.1)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from albucore==0.0.13->paddleocr) (4.11.0.86)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from albumentations==1.4.10->paddleocr) (2.10.6)\n",
            "Requirement already satisfied: termcolor in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from fire>=0.3.0->paddleocr) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from scikit-image->paddleocr) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from scikit-image->paddleocr) (2025.1.10)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from scikit-image->paddleocr) (0.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from beautifulsoup4->paddleocr) (2.6)\n",
            "Requirement already satisfied: anyio in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from httpx->paddlepaddle) (4.8.0)\n",
            "Requirement already satisfied: certifi in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from httpx->paddlepaddle) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from httpx->paddlepaddle) (1.0.7)\n",
            "Requirement already satisfied: idna in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from httpx->paddlepaddle) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from httpcore==1.*->httpx->paddlepaddle) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from python-docx->paddleocr) (5.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from pydantic>=2.7.0->albumentations==1.4.10->paddleocr) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from pydantic>=2.7.0->albumentations==1.4.10->paddleocr) (2.27.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from anyio->httpx->paddlepaddle) (1.3.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "# If you already have these packages installed, you can skip this step\n",
        "!pip install torch transformers pdf2image pillow matplotlib opencv-python tqdm scikit-learn pandas paddlepaddle paddleocr\n",
        "\n",
        "# or !pip install -r layoutlmv3/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Check if poppler is installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: poppler-utils in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (0.1.0)\n",
            "Requirement already satisfied: Click>=7.0 in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from poppler-utils) (8.1.8)\n",
            "Requirement already satisfied: colorama in c:\\freelance\\leon\\law bundling poc\\venv\\lib\\site-packages (from Click>=7.0->poppler-utils) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Poppler is installed correctly!\n"
          ]
        }
      ],
      "source": [
        "from pdf2image import convert_from_path\n",
        "print(\"Poppler is installed correctly!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1Mo-zlP6_IV",
        "outputId": "39edaf6b-d071-44ea-de98-aa7f5b6b8c95"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'apt-get' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'apt-get' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYgWN-I67V8g",
        "outputId": "4019abc9-71c7-4ee0-cfec-b832aae62d75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/bin/pdfinfo\n"
          ]
        }
      ],
      "source": [
        "!which pdfinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR4t8gBIR39P",
        "outputId": "c622f150-d36a-4081-82a3-2ccb2f83f381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Volume in drive C has no label.\n",
            " Volume Serial Number is DC6B-7916\n",
            "\n",
            " Directory of c:\\freelance\\Leon\\law bundling POC\\layoutlmv3\n",
            "\n",
            "05/04/2025  07:06 PM    <DIR>          .\n",
            "05/04/2025  07:06 PM    <DIR>          ..\n",
            "05/04/2025  07:06 PM            37,323 annotate.py\n",
            "03/04/2025  06:03 PM             5,922 dataset.py\n",
            "03/04/2025  06:03 PM             7,699 debug_environment.py\n",
            "05/04/2025  02:24 AM            29,552 inference.py\n",
            "03/04/2025  07:06 PM            21,837 notebook.ipynb\n",
            "05/04/2025  07:40 PM           121,479 notebook_v2.ipynb\n",
            "03/04/2025  06:03 PM             7,869 prepare_dataset.py\n",
            "03/04/2025  06:03 PM    <DIR>          processor\n",
            "03/04/2025  06:03 PM             8,510 README.md\n",
            "03/04/2025  06:03 PM               215 requirements.txt\n",
            "03/04/2025  06:03 PM             2,335 setup_processor.py\n",
            "05/04/2025  12:13 AM            12,842 train.py\n",
            "03/04/2025  06:03 PM             9,422 validate_annotations.py\n",
            "              12 File(s)        265,005 bytes\n",
            "               3 Dir(s)  44,404,711,424 bytes free\n"
          ]
        }
      ],
      "source": [
        "# Basic listing of files and directories - linux\n",
        "# !ls\n",
        "\n",
        "#windows\n",
        "!dir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyCoYQxE7ROL"
      },
      "source": [
        "### 1.3 Verify Directory Structure\n",
        "\n",
        "Let's verify the necessary directory structure exists for our project. If not, we'll create the missing directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGwuNgWH7ROM",
        "outputId": "7c94a4da-7ed1-4a90-e68f-ec3dbd3921c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All necessary directories are present.\n",
            "✅ All required files are present.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the expected directory structure\n",
        "directories = [\n",
        "    \"CUAD_v1/layoutlmv3\",\n",
        "    \"CUAD_v1/layoutlmv3_dataset\",\n",
        "    \"CUAD_v1/full_contract_pdf\",\n",
        "    \"layoutlmv3/processor\"\n",
        "]\n",
        "\n",
        "# Verify directories exist\n",
        "missing_dirs = [d for d in directories if not os.path.exists(d)]\n",
        "if missing_dirs:\n",
        "    print(f\"⚠️ Missing directories:\")\n",
        "    for d in missing_dirs:\n",
        "        print(f\"  - {d}\")\n",
        "else:\n",
        "    print(\"✅ All necessary directories are present.\")\n",
        "\n",
        "# Define all required files (based on the screenshot)\n",
        "required_files = [\n",
        "    \"layoutlmv3/annotate.py\",\n",
        "    \"layoutlmv3/dataset.py\",\n",
        "    \"layoutlmv3/debug_environment.py\",\n",
        "    \"layoutlmv3/inference.py\",\n",
        "    \"layoutlmv3/prepare_dataset.py\",\n",
        "    \"layoutlmv3/requirements.txt\",\n",
        "    \"layoutlmv3/setup_processor.py\",\n",
        "    \"layoutlmv3/train.py\",\n",
        "    \"layoutlmv3/validate_annotations.py\"\n",
        "]\n",
        "\n",
        "# Verify the required files exist\n",
        "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
        "\n",
        "if missing_files:\n",
        "    print(f\"⚠️ Missing {len(missing_files)} required files:\")\n",
        "    for f in missing_files:\n",
        "        print(f\"  - {f}\")\n",
        "else:\n",
        "    print(\"✅ All required files are present.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2KzNZvD7ROM"
      },
      "source": [
        "### Download or Import CUAD Dataset\n",
        "\n",
        "To use this notebook, you need the CUAD dataset. You can either download it directly or upload your own files.\n",
        "\n",
        "**Option 1: Download CUAD dataset** (Note: The dataset is quite large)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8oN6EEv7ROM"
      },
      "outputs": [],
      "source": [
        "# This cell is optional - Run if you need to download the CUAD dataset\n",
        "# Uncomment the following lines if you want to download\n",
        "\n",
        "# Download CUAD dataset (Note: This is a large download ~ 800MB)\n",
        "# !wget -q https://www.atticusprojectai.org/cuad/CUAD_v1.zip\n",
        "# !unzip -q CUAD_v1.zip\n",
        "# !mv CUAD_v1/* CUAD_v1/\n",
        "# TODO: make sure the paths are correct\n",
        "print(\"If you already have the CUAD dataset, please make sure the files are organized in the CUAD_v1 directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIW5_iVu7ROM"
      },
      "source": [
        "**Option 2: Upload your own PDF files** (if you have a specific set of documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVV1ADS77RON"
      },
      "outputs": [],
      "source": [
        "# # This cell is optional - Run if you're uploading your own documents\n",
        "# # Only run if using Colab and you want to upload PDFs manually\n",
        "\n",
        "# if IN_COLAB:\n",
        "#     from google.colab import files\n",
        "\n",
        "#     print(\"Upload your PDF files (you can select multiple files):\")\n",
        "#     uploaded = files.upload()\n",
        "\n",
        "#     # Move uploaded files to the appropriate directory\n",
        "#     for filename in uploaded.keys():\n",
        "#         destination = f\"CUAD_v1/full_contract_pdf/{filename}\"\n",
        "#         !cp \"{filename}\" \"{destination}\"\n",
        "#         print(f\"Moved {filename} to {destination}\")\n",
        "# else:\n",
        "#     print(\"In local environment, please manually place your PDF files in the CUAD_v1/full_contract_pdf directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg6ty-_V7RON"
      },
      "source": [
        "## 2. Data Annotation and Preparation\n",
        "\n",
        "The first step is to annotate the PDF documents and extract spatial information for the text. Then, we'll prepare these annotations for training with LayoutLMv3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d965Twk7RON"
      },
      "source": [
        "### 2.1 Document Annotation\n",
        "\n",
        "The annotation process uses OCR to identify text locations in PDFs and aligns this with the CUAD annotations.\n",
        "\n",
        "**Note:** This step can be time-consuming. If you have already generated annotations, you can skip this step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFnrW_NG7RON",
        "outputId": "a513d2c2-a2d5-4375-d203-50f19ab5e8f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing from Part_I: 5 directories - ['Affiliate_Agreements', 'Co_Branding', 'Development', 'Distributor', 'Endorsement']\n",
            "Found 79 existing annotation files in CUAD_v1/layoutlmv3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-06 10:29:35 - INFO - Starting CUAD dataset processing with improved matching algorithms\n",
            "[2025-04-06 10:29:35,306] [    INFO] annotate.py:715 - Starting CUAD dataset processing with improved matching algorithms\n",
            "2025-04-06 10:29:35 - INFO - Input directory: CUAD_v1/\n",
            "[2025-04-06 10:29:35,307] [    INFO] annotate.py:716 - Input directory: CUAD_v1/\n",
            "2025-04-06 10:29:35 - INFO - Output directory: CUAD_v1/layoutlmv3\n",
            "[2025-04-06 10:29:35,307] [    INFO] annotate.py:717 - Output directory: CUAD_v1/layoutlmv3\n",
            "2025-04-06 10:29:35 - INFO - PDF DPI: 400\n",
            "[2025-04-06 10:29:35,307] [    INFO] annotate.py:718 - PDF DPI: 400\n",
            "2025-04-06 10:29:35 - INFO - Match threshold: 0.6\n",
            "[2025-04-06 10:29:35,307] [    INFO] annotate.py:719 - Match threshold: 0.6\n",
            "2025-04-06 10:29:35 - INFO - Successfully loaded annotations from CUAD_v1/CUAD_v1.json\n",
            "[2025-04-06 10:29:35,506] [    INFO] annotate.py:743 - Successfully loaded annotations from CUAD_v1/CUAD_v1.json\n",
            "2025-04-06 10:29:35 - INFO - Found 510 documents in annotations\n",
            "[2025-04-06 10:29:35,506] [    INFO] annotate.py:744 - Found 510 documents in annotations\n",
            "2025-04-06 10:29:35 - INFO - Found 80 PDF files to process\n",
            "[2025-04-06 10:29:35,507] [    INFO] annotate.py:776 - Found 80 PDF files to process\n",
            "\n",
            "Processing documents:   0%|          | 0/80 [00:00<?, ?it/s]2025-04-06 10:29:35 - INFO - Skipping CreditcardscomInc_20070810_S-1_EX-10.33_362297_EX-10.33_Affiliate Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,508] [    INFO] annotate.py:791 - Skipping CreditcardscomInc_20070810_S-1_EX-10.33_362297_EX-10.33_Affiliate Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping CybergyHoldingsInc_20140520_10-Q_EX-10.27_8605784_EX-10.27_Affiliate Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,509] [    INFO] annotate.py:791 - Skipping CybergyHoldingsInc_20140520_10-Q_EX-10.27_8605784_EX-10.27_Affiliate Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping DigitalCinemaDestinationsCorp_20111220_S-1_EX-10.10_7346719_EX-10.10_Affiliate Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,509] [    INFO] annotate.py:791 - Skipping DigitalCinemaDestinationsCorp_20111220_S-1_EX-10.10_7346719_EX-10.10_Affiliate Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping LinkPlusCorp_20050802_8-K_EX-10_3240252_EX-10_Affiliate Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,509] [    INFO] annotate.py:791 - Skipping LinkPlusCorp_20050802_8-K_EX-10_3240252_EX-10_Affiliate Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping SouthernStarEnergyInc_20051202_SB-2A_EX-9_801890_EX-9_Affiliate Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,509] [    INFO] annotate.py:791 - Skipping SouthernStarEnergyInc_20051202_SB-2A_EX-9_801890_EX-9_Affiliate Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping SteelVaultCorp_20081224_10-K_EX-10.16_3074935_EX-10.16_Affiliate Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,509] [    INFO] annotate.py:791 - Skipping SteelVaultCorp_20081224_10-K_EX-10.16_3074935_EX-10.16_Affiliate Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping TubeMediaCorp_20060310_8-K_EX-10.1_513921_EX-10.1_Affiliate Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,509] [    INFO] annotate.py:791 - Skipping TubeMediaCorp_20060310_8-K_EX-10.1_513921_EX-10.1_Affiliate Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping UnionDentalHoldingsInc_20050204_8-KA_EX-10_3345577_EX-10_Affiliate Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,509] [    INFO] annotate.py:791 - Skipping UnionDentalHoldingsInc_20050204_8-KA_EX-10_3345577_EX-10_Affiliate Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping UsioInc_20040428_SB-2_EX-10.11_1723988_EX-10.11_Affiliate Agreement 2 - output file already exists\n",
            "[2025-04-06 10:29:35,509] [    INFO] annotate.py:791 - Skipping UsioInc_20040428_SB-2_EX-10.11_1723988_EX-10.11_Affiliate Agreement 2 - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping 2ThemartComInc_19990826_10-12G_EX-10.10_6700288_EX-10.10_Co-Branding Agreement_ Agency Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,509] [    INFO] annotate.py:791 - Skipping 2ThemartComInc_19990826_10-12G_EX-10.10_6700288_EX-10.10_Co-Branding Agreement_ Agency Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping DeltathreeInc_19991102_S-1A_EX-10.19_6227850_EX-10.19_Co-Branding Agreement_ Service Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,509] [    INFO] annotate.py:791 - Skipping DeltathreeInc_19991102_S-1A_EX-10.19_6227850_EX-10.19_Co-Branding Agreement_ Service Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping EbixInc_20010515_10-Q_EX-10.3_4049767_EX-10.3_Co-Branding Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,509] [    INFO] annotate.py:791 - Skipping EbixInc_20010515_10-Q_EX-10.3_4049767_EX-10.3_Co-Branding Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping EdietsComInc_20001030_10QSB_EX-10.4_2606646_EX-10.4_Co-Branding Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,509] [    INFO] annotate.py:791 - Skipping EdietsComInc_20001030_10QSB_EX-10.4_2606646_EX-10.4_Co-Branding Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping EmbarkComInc_19991008_S-1A_EX-10.10_6487661_EX-10.10_Co-Branding Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,509] [    INFO] annotate.py:791 - Skipping EmbarkComInc_19991008_S-1A_EX-10.10_6487661_EX-10.10_Co-Branding Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping HealthcentralCom_19991108_S-1A_EX-10.27_6623292_EX-10.27_Co-Branding Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,510] [    INFO] annotate.py:791 - Skipping HealthcentralCom_19991108_S-1A_EX-10.27_6623292_EX-10.27_Co-Branding Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping ImpresseCorp_20000322_S-1A_EX-10.11_5199234_EX-10.11_Co-Branding Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,510] [    INFO] annotate.py:791 - Skipping ImpresseCorp_20000322_S-1A_EX-10.11_5199234_EX-10.11_Co-Branding Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping IntegrityMediaInc_20010329_10-K405_EX-10.17_2373875_EX-10.17_Co-Branding Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,510] [    INFO] annotate.py:791 - Skipping IntegrityMediaInc_20010329_10-K405_EX-10.17_2373875_EX-10.17_Co-Branding Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping InvendaCorp_20000828_S-1A_EX-10.2_2588206_EX-10.2_Co-Branding Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,510] [    INFO] annotate.py:791 - Skipping InvendaCorp_20000828_S-1A_EX-10.2_2588206_EX-10.2_Co-Branding Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping LeadersonlineInc_20000427_S-1A_EX-10.8_4991089_EX-10.8_Co-Branding Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,510] [    INFO] annotate.py:791 - Skipping LeadersonlineInc_20000427_S-1A_EX-10.8_4991089_EX-10.8_Co-Branding Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping MphaseTechnologiesInc_20030911_10-K_EX-10.15_1560667_EX-10.15_Co-Branding Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,510] [    INFO] annotate.py:791 - Skipping MphaseTechnologiesInc_20030911_10-K_EX-10.15_1560667_EX-10.15_Co-Branding Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping MusclepharmCorp_20170208_10-KA_EX-10.38_9893581_EX-10.38_Co-Branding Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,510] [    INFO] annotate.py:791 - Skipping MusclepharmCorp_20170208_10-KA_EX-10.38_9893581_EX-10.38_Co-Branding Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping NeoformaInc_19991202_S-1A_EX-10.26_5224521_EX-10.26_Co-Branding Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,510] [    INFO] annotate.py:791 - Skipping NeoformaInc_19991202_S-1A_EX-10.26_5224521_EX-10.26_Co-Branding Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping PaperexchangeComInc_20000322_S-1A_EX-10.4_5202103_EX-10.4_Co-Branding Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,510] [    INFO] annotate.py:791 - Skipping PaperexchangeComInc_20000322_S-1A_EX-10.4_5202103_EX-10.4_Co-Branding Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping PcquoteComInc_19990721_S-1A_EX-10.11_6377149_EX-10.11_Co-Branding Agreement1 - output file already exists\n",
            "[2025-04-06 10:29:35,510] [    INFO] annotate.py:791 - Skipping PcquoteComInc_19990721_S-1A_EX-10.11_6377149_EX-10.11_Co-Branding Agreement1 - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping PcquoteComInc_19990721_S-1A_EX-10.11_6377149_EX-10.11_Co-Branding Agreement2 - output file already exists\n",
            "[2025-04-06 10:29:35,510] [    INFO] annotate.py:791 - Skipping PcquoteComInc_19990721_S-1A_EX-10.11_6377149_EX-10.11_Co-Branding Agreement2 - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping PcquoteComInc_19990721_S-1A_EX-10.11_6377149_EX-10.11_Co-Branding Agreement3 - output file already exists\n",
            "[2025-04-06 10:29:35,510] [    INFO] annotate.py:791 - Skipping PcquoteComInc_19990721_S-1A_EX-10.11_6377149_EX-10.11_Co-Branding Agreement3 - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping RaeSystemsInc_20001114_10-Q_EX-10.57_2631790_EX-10.57_Co-Branding Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,510] [    INFO] annotate.py:791 - Skipping RaeSystemsInc_20001114_10-Q_EX-10.57_2631790_EX-10.57_Co-Branding Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping RandWorldwideInc_20010402_8-KA_EX-10.2_2102464_EX-10.2_Co-Branding Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,511] [    INFO] annotate.py:791 - Skipping RandWorldwideInc_20010402_8-KA_EX-10.2_2102464_EX-10.2_Co-Branding Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping StampscomInc_20001114_10-Q_EX-10.47_2631630_EX-10.47_Co-Branding Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,511] [    INFO] annotate.py:791 - Skipping StampscomInc_20001114_10-Q_EX-10.47_2631630_EX-10.47_Co-Branding Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping TheglobeComInc_19990503_S-1A_EX-10.20_5416126_EX-10.20_Co-Branding Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,511] [    INFO] annotate.py:791 - Skipping TheglobeComInc_19990503_S-1A_EX-10.20_5416126_EX-10.20_Co-Branding Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping TomOnlineInc_20060501_20-F_EX-4.46_749700_EX-4.46_Co-Branding Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,511] [    INFO] annotate.py:791 - Skipping TomOnlineInc_20060501_20-F_EX-4.46_749700_EX-4.46_Co-Branding Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping AimmuneTherapeuticsInc_20200205_8-K_EX-10.3_11967170_EX-10.3_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,511] [    INFO] annotate.py:791 - Skipping AimmuneTherapeuticsInc_20200205_8-K_EX-10.3_11967170_EX-10.3_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping ArcaUsTreasuryFund_20200207_N-2_EX-99.K5_11971930_EX-99.K5_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,511] [    INFO] annotate.py:791 - Skipping ArcaUsTreasuryFund_20200207_N-2_EX-99.K5_11971930_EX-99.K5_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping ClickstreamCorp_20200330_1-A_EX1A-6 MAT CTRCT_12089935_EX1A-6 MAT CTRCT_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,512] [    INFO] annotate.py:791 - Skipping ClickstreamCorp_20200330_1-A_EX1A-6 MAT CTRCT_12089935_EX1A-6 MAT CTRCT_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping CnsPharmaceuticalsInc_20200326_8-K_EX-10.1_12079626_EX-10.1_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,512] [    INFO] annotate.py:791 - Skipping CnsPharmaceuticalsInc_20200326_8-K_EX-10.1_12079626_EX-10.1_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping CoherusBiosciencesInc_20200227_10-K_EX-10.29_12021376_EX-10.29_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,512] [    INFO] annotate.py:791 - Skipping CoherusBiosciencesInc_20200227_10-K_EX-10.29_12021376_EX-10.29_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping ConformisInc_20191101_10-Q_EX-10.6_11861402_EX-10.6_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,512] [    INFO] annotate.py:791 - Skipping ConformisInc_20191101_10-Q_EX-10.6_11861402_EX-10.6_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping ElPolloLocoHoldingsInc_20200306_10-K_EX-10.16_12041700_EX-10.16_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,512] [    INFO] annotate.py:791 - Skipping ElPolloLocoHoldingsInc_20200306_10-K_EX-10.16_12041700_EX-10.16_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping EmeraldHealthBioceuticalsInc_20200218_1-A_EX1A-6 MAT CTRCT_11987205_EX1A-6 MAT CTRCT_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,512] [    INFO] annotate.py:791 - Skipping EmeraldHealthBioceuticalsInc_20200218_1-A_EX1A-6 MAT CTRCT_11987205_EX1A-6 MAT CTRCT_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping EtonPharmaceuticalsInc_20191114_10-Q_EX-10.1_11893941_EX-10.1_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,512] [    INFO] annotate.py:791 - Skipping EtonPharmaceuticalsInc_20191114_10-Q_EX-10.1_11893941_EX-10.1_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping FuelcellEnergyInc_20191106_8-K_EX-10.1_11868007_EX-10.1_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,512] [    INFO] annotate.py:791 - Skipping FuelcellEnergyInc_20191106_8-K_EX-10.1_11868007_EX-10.1_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - ERROR - Annotation not found for PDF: CUAD_v1/full_contract_pdf\\Part_I\\Development\\HarpoonTherapeuticsInc_20200312_10-K_EX-10.18_12051356_EX-10.18_Development Agreement_Option Agreement.pdf\n",
            "[2025-04-06 10:29:35,512] [   ERROR] annotate.py:822 - Annotation not found for PDF: CUAD_v1/full_contract_pdf\\Part_I\\Development\\HarpoonTherapeuticsInc_20200312_10-K_EX-10.18_12051356_EX-10.18_Development Agreement_Option Agreement.pdf\n",
            "2025-04-06 10:29:35 - INFO - Skipping HfEnterprisesInc_20191223_S-1_EX-10.22_11931299_EX-10.22_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,512] [    INFO] annotate.py:791 - Skipping HfEnterprisesInc_20191223_S-1_EX-10.22_11931299_EX-10.22_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping IbioInc_20200313_8-K_EX-10.1_12052678_EX-10.1_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,512] [    INFO] annotate.py:791 - Skipping IbioInc_20200313_8-K_EX-10.1_12052678_EX-10.1_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping LegacyEducationAllianceInc_20200330_10-K_EX-10.18_12090678_EX-10.18_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,512] [    INFO] annotate.py:791 - Skipping LegacyEducationAllianceInc_20200330_10-K_EX-10.18_12090678_EX-10.18_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping LiquidmetalTechnologiesInc_20200205_8-K_EX-10.1_11968198_EX-10.1_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,513] [    INFO] annotate.py:791 - Skipping LiquidmetalTechnologiesInc_20200205_8-K_EX-10.1_11968198_EX-10.1_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping NlsPharmaceuticsLtd_20200228_F-1_EX-10.14_12029046_EX-10.14_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,513] [    INFO] annotate.py:791 - Skipping NlsPharmaceuticsLtd_20200228_F-1_EX-10.14_12029046_EX-10.14_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping PelicanDeliversInc_20200211_S-1_EX-10.3_11975895_EX-10.3_Development Agreement1 - output file already exists\n",
            "[2025-04-06 10:29:35,513] [    INFO] annotate.py:791 - Skipping PelicanDeliversInc_20200211_S-1_EX-10.3_11975895_EX-10.3_Development Agreement1 - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping PelicanDeliversInc_20200211_S-1_EX-10.3_11975895_EX-10.3_Development Agreement2 - output file already exists\n",
            "[2025-04-06 10:29:35,513] [    INFO] annotate.py:791 - Skipping PelicanDeliversInc_20200211_S-1_EX-10.3_11975895_EX-10.3_Development Agreement2 - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping PhasebioPharmaceuticalsInc_20200330_10-K_EX-10.21_12086810_EX-10.21_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,513] [    INFO] annotate.py:791 - Skipping PhasebioPharmaceuticalsInc_20200330_10-K_EX-10.21_12086810_EX-10.21_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping ReedsInc_20191113_10-Q_EX-10.4_11888303_EX-10.4_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,513] [    INFO] annotate.py:791 - Skipping ReedsInc_20191113_10-Q_EX-10.4_11888303_EX-10.4_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping RevolutionMedicinesInc_20200117_S-1_EX-10.1_11948417_EX-10.1_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,513] [    INFO] annotate.py:791 - Skipping RevolutionMedicinesInc_20200117_S-1_EX-10.1_11948417_EX-10.1_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping RitterPharmaceuticalsInc_20200313_S-4A_EX-10.54_12055220_EX-10.54_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,513] [    INFO] annotate.py:791 - Skipping RitterPharmaceuticalsInc_20200313_S-4A_EX-10.54_12055220_EX-10.54_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping VgrabCommunicationsInc_20200129_10-K_EX-10.33_11958828_EX-10.33_Development Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,513] [    INFO] annotate.py:791 - Skipping VgrabCommunicationsInc_20200129_10-K_EX-10.33_11958828_EX-10.33_Development Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping FuseMedicalInc_20190321_10-K_EX-10.43_11575454_EX-10.43_Distributor Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,513] [    INFO] annotate.py:791 - Skipping FuseMedicalInc_20190321_10-K_EX-10.43_11575454_EX-10.43_Distributor Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping GentechHoldingsInc_20190808_1-A_EX1A-6 MAT CTRCT_11776814_EX1A-6 MAT CTRCT_Distributor Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,513] [    INFO] annotate.py:791 - Skipping GentechHoldingsInc_20190808_1-A_EX1A-6 MAT CTRCT_11776814_EX1A-6 MAT CTRCT_Distributor Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping ImineCorp_20180725_S-1_EX-10.5_11275970_EX-10.5_Distributor Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,513] [    INFO] annotate.py:791 - Skipping ImineCorp_20180725_S-1_EX-10.5_11275970_EX-10.5_Distributor Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping InnerscopeHearingTechnologiesInc_20181109_8-K_EX-10.6_11419704_EX-10.6_Distributor Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,514] [    INFO] annotate.py:791 - Skipping InnerscopeHearingTechnologiesInc_20181109_8-K_EX-10.6_11419704_EX-10.6_Distributor Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping PrecheckHealthServicesInc_20200320_8-K_EX-99.2_12070169_EX-99.2_Distributor Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,514] [    INFO] annotate.py:791 - Skipping PrecheckHealthServicesInc_20200320_8-K_EX-99.2_12070169_EX-99.2_Distributor Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping ScansourceInc_20190509_10-Q_EX-10.2_11661422_EX-10.2_Distributor Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,514] [    INFO] annotate.py:791 - Skipping ScansourceInc_20190509_10-Q_EX-10.2_11661422_EX-10.2_Distributor Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping ScansourceInc_20190822_10-K_EX-10.38_11793958_EX-10.38_Distributor Agreement1 - output file already exists\n",
            "[2025-04-06 10:29:35,514] [    INFO] annotate.py:791 - Skipping ScansourceInc_20190822_10-K_EX-10.38_11793958_EX-10.38_Distributor Agreement1 - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping ScansourceInc_20190822_10-K_EX-10.38_11793958_EX-10.38_Distributor Agreement2 - output file already exists\n",
            "[2025-04-06 10:29:35,514] [    INFO] annotate.py:791 - Skipping ScansourceInc_20190822_10-K_EX-10.38_11793958_EX-10.38_Distributor Agreement2 - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping ScansourceInc_20190822_10-K_EX-10.39_11793959_EX-10.39_Distributor Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,514] [    INFO] annotate.py:791 - Skipping ScansourceInc_20190822_10-K_EX-10.39_11793959_EX-10.39_Distributor Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping SmartRxSystemsInc_20180914_1-A_EX1A-6 MAT CTRCT_11351705_EX1A-6 MAT CTRCT_Distributor Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,514] [    INFO] annotate.py:791 - Skipping SmartRxSystemsInc_20180914_1-A_EX1A-6 MAT CTRCT_11351705_EX1A-6 MAT CTRCT_Distributor Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping StaarSurgicalCompany_20180801_10-Q_EX-10.37_11289449_EX-10.37_Distributor Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,514] [    INFO] annotate.py:791 - Skipping StaarSurgicalCompany_20180801_10-Q_EX-10.37_11289449_EX-10.37_Distributor Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping WaterNowInc_20191120_10-Q_EX-10.12_11900227_EX-10.12_Distributor Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,514] [    INFO] annotate.py:791 - Skipping WaterNowInc_20191120_10-Q_EX-10.12_11900227_EX-10.12_Distributor Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping ZogenixInc_20190509_10-Q_EX-10.2_11663313_EX-10.2_Distributor Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,514] [    INFO] annotate.py:791 - Skipping ZogenixInc_20190509_10-Q_EX-10.2_11663313_EX-10.2_Distributor Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping BerkshireHillsBancorpInc_20120809_10-Q_EX-10.16_7708169_EX-10.16_Endorsement Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,514] [    INFO] annotate.py:791 - Skipping BerkshireHillsBancorpInc_20120809_10-Q_EX-10.16_7708169_EX-10.16_Endorsement Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping BizzingoInc_20120322_8-K_EX-10.17_7504499_EX-10.17_Endorsement Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,514] [    INFO] annotate.py:791 - Skipping BizzingoInc_20120322_8-K_EX-10.17_7504499_EX-10.17_Endorsement Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping EcoScienceSolutionsInc_20171117_8-K_EX-10.1_10956472_EX-10.1_Endorsement Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,514] [    INFO] annotate.py:791 - Skipping EcoScienceSolutionsInc_20171117_8-K_EX-10.1_10956472_EX-10.1_Endorsement Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping GridironBionutrientsInc_20171206_8-K_EX-10.1_10972555_EX-10.1_Endorsement Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,514] [    INFO] annotate.py:791 - Skipping GridironBionutrientsInc_20171206_8-K_EX-10.1_10972555_EX-10.1_Endorsement Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping GridironBionutrientsInc_20171206_8-K_EX-10.2_10972556_EX-10.2_Endorsement Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,515] [    INFO] annotate.py:791 - Skipping GridironBionutrientsInc_20171206_8-K_EX-10.2_10972556_EX-10.2_Endorsement Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping LegacyEducationAllianceInc_20141110_8-K_EX-10.9_8828866_EX-10.9_Endorsement Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,515] [    INFO] annotate.py:791 - Skipping LegacyEducationAllianceInc_20141110_8-K_EX-10.9_8828866_EX-10.9_Endorsement Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping LifewayFoodsInc_20160316_10-K_EX-10.24_9489766_EX-10.24_Endorsement Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,515] [    INFO] annotate.py:791 - Skipping LifewayFoodsInc_20160316_10-K_EX-10.24_9489766_EX-10.24_Endorsement Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping NakedBrandGroupInc_20150731_POS AM (on S-1)_EX-10.75_9196027_EX-10.75_Endorsement Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,515] [    INFO] annotate.py:791 - Skipping NakedBrandGroupInc_20150731_POS AM (on S-1)_EX-10.75_9196027_EX-10.75_Endorsement Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping PapaJohnsInternationalInc_20190617_8-K_EX-10.1_11707365_EX-10.1_Endorsement Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,515] [    INFO] annotate.py:791 - Skipping PapaJohnsInternationalInc_20190617_8-K_EX-10.1_11707365_EX-10.1_Endorsement Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping PerformanceSportsBrandsInc_20110909_S-1_EX-10.10_7220214_EX-10.10_Endorsement Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,515] [    INFO] annotate.py:791 - Skipping PerformanceSportsBrandsInc_20110909_S-1_EX-10.10_7220214_EX-10.10_Endorsement Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping PharmagenInc_20120803_8-KA_EX-10.1_7693204_EX-10.1_Endorsement Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,515] [    INFO] annotate.py:791 - Skipping PharmagenInc_20120803_8-KA_EX-10.1_7693204_EX-10.1_Endorsement Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping PrudentialBancorpInc_20170606_8-K_EX-10.4_10474434_EX-10.4_Endorsement Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,515] [    INFO] annotate.py:791 - Skipping PrudentialBancorpInc_20170606_8-K_EX-10.4_10474434_EX-10.4_Endorsement Agreement - output file already exists\n",
            "2025-04-06 10:29:35 - INFO - Skipping ThriventVariableInsuranceAccountB_20190701_N-6_EX-99.D(IV)_11720968_EX-99.D(IV)_Endorsement Agreement - output file already exists\n",
            "[2025-04-06 10:29:35,515] [    INFO] annotate.py:791 - Skipping ThriventVariableInsuranceAccountB_20190701_N-6_EX-99.D(IV)_11720968_EX-99.D(IV)_Endorsement Agreement - output file already exists\n",
            "\n",
            "Processing documents: 100%|██████████| 80/80 [00:00<00:00, 12297.76it/s]\n",
            "2025-04-06 10:29:35 - INFO - ========== CUAD Dataset Processing Summary ==========\n",
            "[2025-04-06 10:29:35,515] [    INFO] annotate.py:830 - ========== CUAD Dataset Processing Summary ==========\n",
            "2025-04-06 10:29:35 - INFO - Documents processed: 0/1 (0.00% success rate)\n",
            "[2025-04-06 10:29:35,515] [    INFO] annotate.py:831 - Documents processed: 0/1 (0.00% success rate)\n",
            "2025-04-06 10:29:35 - INFO - Documents skipped: 79\n",
            "[2025-04-06 10:29:35,516] [    INFO] annotate.py:833 - Documents skipped: 79\n",
            "2025-04-06 10:29:35 - INFO - Annotations matched: 0/0 (0.00% success rate)\n",
            "[2025-04-06 10:29:35,516] [    INFO] annotate.py:834 - Annotations matched: 0/0 (0.00% success rate)\n",
            "2025-04-06 10:29:35 - INFO - Total processing time: 0.21s\n",
            "[2025-04-06 10:29:35,516] [    INFO] annotate.py:835 - Total processing time: 0.21s\n",
            "2025-04-06 10:29:35 - INFO - Average time per document: 0.21s\n",
            "[2025-04-06 10:29:35,516] [    INFO] annotate.py:836 - Average time per document: 0.21s\n",
            "2025-04-06 10:29:35 - INFO - ====================================================\n",
            "[2025-04-06 10:29:35,516] [    INFO] annotate.py:837 - ====================================================\n"
          ]
        }
      ],
      "source": [
        "# Run document annotation\n",
        "# This process might take several hours depending on dataset size\n",
        "# Uncomment the following lines if you want to run the annotation process\n",
        "# limit: the amount of documents\n",
        "\n",
        "!python layoutlmv3/annotate.py --target-part=\"Part_I\" --num-dirs=5 --skip-existing\n",
        "# Check if annotations exist\n",
        "if os.path.exists(\"CUAD_v1/layoutlmv3\"):\n",
        "    annotation_files = [f for f in os.listdir(\"CUAD_v1/layoutlmv3\") if f.endswith('_layoutlm.json')]\n",
        "    if annotation_files:\n",
        "        print(f\"Found {len(annotation_files)} existing annotation files in CUAD_v1/layoutlmv3\")\n",
        "    else:\n",
        "        print(\"No annotation files found. You may need to run the annotation process.\")\n",
        "else:\n",
        "    print(\"Annotation directory not found. Please run the annotation process or provide pre-annotated files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking files from Part_I: 5 directories - ['Affiliate_Agreements', 'Co_Branding', 'Development', 'Distributor', 'Endorsement']\n",
            "❌ Missing annotation for: HarpoonTherapeuticsInc_20200312_10-K_EX-10.18_12051356_EX-10.18_Development Agreement_Option Agreement\n",
            "\n",
            "=== Annotation Status ===\n",
            "Total PDFs to annotate: 80\n",
            "Successfully annotated: 79\n",
            "Missing annotations: 1\n",
            "Completion rate: 98.75%\n",
            "\n",
            "PDFs still needing annotation:\n",
            "1. HarpoonTherapeuticsInc_20200312_10-K_EX-10.18_12051356_EX-10.18_Development Agreement_Option Agreement.pdf\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Define paths\n",
        "CUAD_DIR = \"CUAD_v1/\"\n",
        "PDF_DIR = os.path.join(CUAD_DIR, \"full_contract_pdf\")\n",
        "ANNOTATION_DIR = os.path.join(CUAD_DIR, \"layoutlmv3\")\n",
        "TARGET_PART = \"Part_I\"\n",
        "NUM_DIRS = 5\n",
        "\n",
        "def find_target_pdfs():\n",
        "    \"\"\"Find all PDFs that should have been annotated based on target part and num_dirs\"\"\"\n",
        "    pdf_files = []\n",
        "    part_path = os.path.join(PDF_DIR, TARGET_PART)\n",
        "    \n",
        "    if not os.path.exists(part_path):\n",
        "        print(f\"Warning: {part_path} does not exist\")\n",
        "        return pdf_files\n",
        "    \n",
        "    # Get all subdirectories in the part\n",
        "    subdirs = [d for d in os.listdir(part_path) \n",
        "              if os.path.isdir(os.path.join(part_path, d))]\n",
        "    \n",
        "    # Sort them and take first NUM_DIRS\n",
        "    subdirs.sort()\n",
        "    target_dirs = subdirs[:NUM_DIRS]\n",
        "    \n",
        "    print(f\"Checking files from {TARGET_PART}: {len(target_dirs)} directories - {target_dirs}\")\n",
        "    \n",
        "    # Find all PDFs in these directories\n",
        "    for subdir in target_dirs:\n",
        "        subdir_path = os.path.join(part_path, subdir)\n",
        "        for file in os.listdir(subdir_path):\n",
        "            if file.endswith(\".pdf\"):\n",
        "                pdf_path = os.path.join(subdir_path, file)\n",
        "                pdf_files.append(pdf_path)\n",
        "    \n",
        "    return pdf_files\n",
        "\n",
        "def check_annotations():\n",
        "    \"\"\"Check if all target PDFs have corresponding annotation files\"\"\"\n",
        "    pdf_files = find_target_pdfs()\n",
        "    \n",
        "    if not pdf_files:\n",
        "        print(\"No PDF files found to check\")\n",
        "        return\n",
        "    \n",
        "    total_pdfs = len(pdf_files)\n",
        "    annotated_pdfs = 0\n",
        "    missing_annotations = []\n",
        "    \n",
        "    for pdf_path in pdf_files:\n",
        "        pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "        annotation_path = os.path.join(ANNOTATION_DIR, f\"{pdf_name}_layoutlm.json\")\n",
        "        \n",
        "        if os.path.exists(annotation_path):\n",
        "            # Check if the annotation file has valid content\n",
        "            try:\n",
        "                with open(annotation_path, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "                    if 'annotations' in data and len(data['annotations']) > 0:\n",
        "                        annotated_pdfs += 1\n",
        "                    else:\n",
        "                        missing_annotations.append(pdf_path)\n",
        "                        print(f\"⚠️ Empty annotations for: {pdf_name}\")\n",
        "            except:\n",
        "                missing_annotations.append(pdf_path)\n",
        "                print(f\"❌ Invalid annotation file for: {pdf_name}\")\n",
        "        else:\n",
        "            missing_annotations.append(pdf_path)\n",
        "            print(f\"❌ Missing annotation for: {pdf_name}\")\n",
        "    \n",
        "    completion_rate = (annotated_pdfs / total_pdfs) * 100 if total_pdfs > 0 else 0\n",
        "    \n",
        "    print(\"\\n=== Annotation Status ===\")\n",
        "    print(f\"Total PDFs to annotate: {total_pdfs}\")\n",
        "    print(f\"Successfully annotated: {annotated_pdfs}\")\n",
        "    print(f\"Missing annotations: {total_pdfs - annotated_pdfs}\")\n",
        "    print(f\"Completion rate: {completion_rate:.2f}%\")\n",
        "    \n",
        "    if missing_annotations:\n",
        "        print(\"\\nPDFs still needing annotation:\")\n",
        "        for i, pdf_path in enumerate(missing_annotations[:10], 1):\n",
        "            print(f\"{i}. {os.path.basename(pdf_path)}\")\n",
        "        \n",
        "        if len(missing_annotations) > 10:\n",
        "            print(f\"... and {len(missing_annotations) - 10} more\")\n",
        "    else:\n",
        "        print(\"\\n✅ All PDFs have been successfully annotated!\")\n",
        "\n",
        "# Run the check\n",
        "check_annotations()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXpUDbNY7RON"
      },
      "source": [
        "### 2.2 Prepare Dataset for LayoutLMv3 - Run this only if we need to prepare the dataset for the first time\n",
        "\n",
        "Next, we'll convert the annotated documents into the format expected by LayoutLMv3 and split them into training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "40gfL8pJ7RON"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset not found. Running dataset preparation...\n",
            "Preparing dataset for LayoutLMv3 fine-tuning...\n",
            "Splitting dataset: 55 train, 11 validation, 13 test\n",
            "Processing training set...\n",
            "Processing validation set...\n",
            "Processing test set...\n",
            "Dataset preparation complete. Output saved to CUAD_v1/layoutlmv3_dataset\n",
            "Train set: 237 images, 237 annotation files\n",
            "Val set: 49 images, 49 annotation files\n",
            "Test set: 53 images, 53 annotation files\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/55 [00:00<?, ?it/s]\n",
            "  2%|▏         | 1/55 [00:02<02:19,  2.59s/it]\n",
            "  4%|▎         | 2/55 [00:04<01:52,  2.13s/it]\n",
            "  5%|▌         | 3/55 [00:06<02:00,  2.32s/it]\n",
            "  7%|▋         | 4/55 [01:13<23:35, 27.75s/it]\n",
            "  9%|▉         | 5/55 [01:16<15:46, 18.92s/it]\n",
            " 11%|█         | 6/55 [01:36<15:30, 18.98s/it]\n",
            " 13%|█▎        | 7/55 [01:41<11:31, 14.40s/it]\n",
            " 15%|█▍        | 8/55 [01:51<10:25, 13.31s/it]\n",
            " 16%|█▋        | 9/55 [02:08<10:57, 14.30s/it]\n",
            " 18%|█▊        | 10/55 [02:09<07:34, 10.09s/it]\n",
            " 20%|██        | 11/55 [02:11<05:37,  7.67s/it]\n",
            " 22%|██▏       | 12/55 [02:15<04:45,  6.64s/it]\n",
            " 24%|██▎       | 13/55 [02:20<04:18,  6.15s/it]\n",
            " 25%|██▌       | 14/55 [02:25<03:54,  5.73s/it]\n",
            " 27%|██▋       | 15/55 [02:26<02:59,  4.48s/it]\n",
            " 29%|██▉       | 16/55 [02:37<04:03,  6.26s/it]\n",
            " 31%|███       | 17/55 [02:38<03:01,  4.78s/it]\n",
            " 33%|███▎      | 18/55 [02:40<02:18,  3.74s/it]\n",
            " 35%|███▍      | 19/55 [02:46<02:40,  4.46s/it]\n",
            " 36%|███▋      | 20/55 [02:54<03:13,  5.52s/it]\n",
            " 38%|███▊      | 21/55 [02:54<02:18,  4.08s/it]\n",
            " 40%|████      | 22/55 [03:01<02:43,  4.96s/it]\n",
            " 42%|████▏     | 23/55 [03:21<04:57,  9.29s/it]\n",
            " 44%|████▎     | 24/55 [03:23<03:43,  7.19s/it]\n",
            " 45%|████▌     | 25/55 [03:24<02:43,  5.45s/it]\n",
            " 47%|████▋     | 26/55 [03:27<02:12,  4.57s/it]\n",
            " 49%|████▉     | 27/55 [03:28<01:37,  3.48s/it]\n",
            " 51%|█████     | 28/55 [03:30<01:23,  3.10s/it]\n",
            " 53%|█████▎    | 29/55 [03:31<01:01,  2.38s/it]\n",
            " 55%|█████▍    | 30/55 [03:33<00:54,  2.17s/it]\n",
            " 56%|█████▋    | 31/55 [03:34<00:46,  1.92s/it]\n",
            " 58%|█████▊    | 32/55 [03:34<00:34,  1.50s/it]\n",
            " 60%|██████    | 33/55 [04:29<06:20, 17.31s/it]\n",
            " 62%|██████▏   | 34/55 [04:30<04:25, 12.63s/it]\n",
            " 64%|██████▎   | 35/55 [04:34<03:21, 10.07s/it]\n",
            " 65%|██████▌   | 36/55 [04:37<02:27,  7.78s/it]\n",
            " 67%|██████▋   | 37/55 [04:38<01:45,  5.86s/it]\n",
            " 69%|██████▉   | 38/55 [04:41<01:22,  4.87s/it]\n",
            " 71%|███████   | 39/55 [04:42<01:02,  3.92s/it]\n",
            " 73%|███████▎  | 40/55 [04:47<01:02,  4.14s/it]\n",
            " 75%|███████▍  | 41/55 [04:51<00:58,  4.15s/it]\n",
            " 76%|███████▋  | 42/55 [04:53<00:44,  3.42s/it]\n",
            " 78%|███████▊  | 43/55 [04:55<00:35,  2.99s/it]\n",
            " 80%|████████  | 44/55 [04:57<00:29,  2.67s/it]\n",
            " 82%|████████▏ | 45/55 [05:01<00:30,  3.07s/it]\n",
            " 84%|████████▎ | 46/55 [05:04<00:27,  3.05s/it]\n",
            " 85%|████████▌ | 47/55 [05:09<00:29,  3.63s/it]\n",
            " 87%|████████▋ | 48/55 [05:10<00:19,  2.75s/it]\n",
            " 89%|████████▉ | 49/55 [05:17<00:24,  4.13s/it]\n",
            " 91%|█████████ | 50/55 [05:25<00:26,  5.26s/it]\n",
            " 93%|█████████▎| 51/55 [05:26<00:16,  4.10s/it]\n",
            " 95%|█████████▍| 52/55 [05:29<00:11,  3.82s/it]\n",
            " 96%|█████████▋| 53/55 [05:38<00:10,  5.15s/it]\n",
            " 98%|█████████▊| 54/55 [05:39<00:04,  4.00s/it]\n",
            "100%|██████████| 55/55 [05:50<00:00,  6.21s/it]\n",
            "100%|██████████| 55/55 [05:50<00:00,  6.38s/it]\n",
            "\n",
            "  0%|          | 0/11 [00:00<?, ?it/s]\n",
            "  9%|▉         | 1/11 [00:02<00:25,  2.57s/it]\n",
            " 18%|█▊        | 2/11 [00:03<00:12,  1.34s/it]\n",
            " 27%|██▋       | 3/11 [00:11<00:35,  4.45s/it]\n",
            " 36%|███▋      | 4/11 [00:14<00:29,  4.18s/it]\n",
            " 45%|████▌     | 5/11 [00:46<01:23, 13.98s/it]\n",
            " 55%|█████▍    | 6/11 [00:49<00:51, 10.27s/it]\n",
            " 64%|██████▎   | 7/11 [00:51<00:30,  7.67s/it]\n",
            " 73%|███████▎  | 8/11 [00:55<00:18,  6.33s/it]\n",
            " 82%|████████▏ | 9/11 [00:57<00:10,  5.21s/it]\n",
            " 91%|█████████ | 10/11 [00:58<00:03,  3.82s/it]\n",
            "100%|██████████| 11/11 [01:01<00:00,  3.64s/it]\n",
            "100%|██████████| 11/11 [01:01<00:00,  5.62s/it]\n",
            "\n",
            "  0%|          | 0/13 [00:00<?, ?it/s]\n",
            "  8%|▊         | 1/13 [00:01<00:23,  1.94s/it]\n",
            " 15%|█▌        | 2/13 [00:03<00:17,  1.61s/it]\n",
            " 23%|██▎       | 3/13 [00:14<00:57,  5.79s/it]\n",
            " 31%|███       | 4/13 [00:18<00:48,  5.35s/it]\n",
            " 38%|███▊      | 5/13 [00:19<00:30,  3.80s/it]\n",
            " 46%|████▌     | 6/13 [00:22<00:24,  3.47s/it]\n",
            " 54%|█████▍    | 7/13 [00:24<00:18,  3.00s/it]\n",
            " 62%|██████▏   | 8/13 [00:59<01:06, 13.26s/it]\n",
            " 69%|██████▉   | 9/13 [01:03<00:41, 10.31s/it]\n",
            " 77%|███████▋  | 10/13 [01:05<00:22,  7.60s/it]\n",
            " 85%|████████▍ | 11/13 [01:06<00:11,  5.61s/it]\n",
            " 92%|█████████▏| 12/13 [01:07<00:04,  4.19s/it]\n",
            "100%|██████████| 13/13 [01:11<00:00,  4.07s/it]\n",
            "100%|██████████| 13/13 [01:11<00:00,  5.47s/it]\n"
          ]
        }
      ],
      "source": [
        "# Check if dataset already exists\n",
        "dataset_exists = False\n",
        "if os.path.exists(\"CUAD_v1/layoutlmv3_dataset\"):\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        split_dir = f\"CUAD_v1/layoutlmv3_dataset/{split}\"\n",
        "        if os.path.exists(split_dir) and len(os.listdir(split_dir)) > 0:\n",
        "            dataset_exists = True\n",
        "            break #break if the splits already existed\n",
        "\n",
        "if dataset_exists:\n",
        "    print(\"Dataset already prepared. Skipping dataset preparation step.\")\n",
        "    # Show dataset statistics\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        split_dir = f\"CUAD_v1/layoutlmv3_dataset/{split}\"\n",
        "        if os.path.exists(split_dir):\n",
        "            files = os.listdir(split_dir)\n",
        "            images = [f for f in files if f.endswith('.jpg')]\n",
        "            annotations = [f for f in files if f.endswith('.json')]\n",
        "            print(f\"{split.capitalize()} set: {len(images)} images, {len(annotations)} annotation files\")\n",
        "else:\n",
        "    print(\"Dataset not found. Running dataset preparation...\")\n",
        "    # Run dataset preparation\n",
        "    !python layoutlmv3/prepare_dataset.py\n",
        "\n",
        "    # Verify dataset was created\n",
        "    if os.path.exists(\"CUAD_v1/layoutlmv3_dataset\"):\n",
        "        for split in [\"train\", \"val\", \"test\"]:\n",
        "            split_dir = f\"CUAD_v1/layoutlmv3_dataset/{split}\"\n",
        "            if os.path.exists(split_dir):\n",
        "                files = os.listdir(split_dir)\n",
        "                images = [f for f in files if f.endswith('.jpg')]\n",
        "                annotations = [f for f in files if f.endswith('.json')]\n",
        "                print(f\"{split.capitalize()} set: {len(images)} images, {len(annotations)} annotation files\")\n",
        "    else:\n",
        "        print(\"Dataset preparation failed. Please check the logs for errors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tEdAoMW7ROO"
      },
      "source": [
        "## 3. Model Training\n",
        "\n",
        "In this section, we'll train the LayoutLMv3 model on our prepared dataset. We'll skip the processor setup since you already have the processor configured in your environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wgln-B17ROO"
      },
      "source": [
        "### 3.1 Verify Processor Configuration\n",
        "\n",
        "First, let's verify that the processor is already set up correctly before proceeding to training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xuesv45P7ROO",
        "outputId": "ffe8893b-3ce5-4a91-8ca9-c37846a6ad48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processor is already set up and ready to use!\n",
            "Processor files: ['merges.txt', 'preprocessor_config.json', 'processor_settings.txt', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json']\n"
          ]
        }
      ],
      "source": [
        "# Check your processor directory to make sure it's properly set up\n",
        "import os\n",
        "if os.path.exists(\"layoutlmv3/processor/tokenizer_config.json\") and \\\n",
        "   os.path.exists(\"layoutlmv3/processor/preprocessor_config.json\"):\n",
        "    print(\"Processor is already set up and ready to use!\")\n",
        "    # List processor files\n",
        "    processor_files = os.listdir(\"layoutlmv3/processor\")\n",
        "    print(f\"Processor files: {processor_files}\")\n",
        "else:\n",
        "    print(\"Processor may not be properly configured. You might need to run setup_processor.py first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttkEaBiH7ROO"
      },
      "source": [
        "### 3.2 Train the Model\n",
        "\n",
        "Now we'll train LayoutLMv3 on our prepared dataset, using the existing processor. This process will take several hours depending on the size of the dataset and your hardware capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AztX13Ci7ROO",
        "outputId": "e160ea9b-ee8a-4a72-e375-789b156530ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# For more detailed GPU information if available\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6VYJPJf7ROO",
        "outputId": "e7a242fb-4b04-4579-8a10-f476aaee2015"
      },
      "outputs": [],
      "source": [
        "# Uncomment and run this cell to start training\n",
        "!python layoutlmv3/train.py --num_epochs 25 --batch_size 2 --learning_rate 5e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4-mwR2x7ROO"
      },
      "source": [
        "### 3.3 Check Training Results\n",
        "\n",
        "Let's check if the training was successful and the model was saved properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpnQc3hHzfz9",
        "outputId": "c94d2cf4-b882-41dd-bc09-05af10ac6b73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 492018\n",
            "-rw------- 1 root root      2706 Apr  4 16:55 config.json\n",
            "-rw------- 1 root root 503822716 Apr  4 16:55 model.safetensors\n",
            "total 492018\n",
            "-rw------- 1 root root      2706 Apr  4 17:08 config.json\n",
            "-rw------- 1 root root 503822716 Apr  4 17:08 model.safetensors\n"
          ]
        }
      ],
      "source": [
        "!ls -la layoutlmv3/model/train-20250404_165409/best_model/\n",
        "!ls -la layoutlmv3/model/train-20250404_165409/final_model/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3RBVy0m7ROO",
        "outputId": "cc8f4e86-2318-4cc0-8550-d1826a7485c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found latest training directory: layoutlmv3/model/train-20250404_165409\n",
            "Training completed successfully! Best model was saved.\n",
            "Model files: ['config.json', 'model.safetensors']\n",
            "Training metrics were saved. You can use them for analysis.\n",
            "\n",
            "Last few lines from training log:\n",
            "\n",
            "Evaluating at step 1300...\n",
            "Evaluation results: {'precision': 0.0027388271257919893, 'recall': 0.05233380480905234, 'f1': 0.005205244026706818}\n",
            "Epoch 25 average loss: 2.9270\n",
            "Evaluating after epoch 25...\n",
            "Evaluation results: {'precision': 0.0027388271257919893, 'recall': 0.05233380480905234, 'f1': 0.005205244026706818}\n",
            "Training complete. Saving final model...\n",
            "Best F1: 0.0568\n",
            "Model saved to layoutlmv3/model/train-20250404_165409\n",
            "Training completed at: 2025-04-04 17:08:57\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Find the most recent training directory\n",
        "train_dirs = sorted(glob.glob(\"layoutlmv3/model/train-*\"), reverse=True)\n",
        "\n",
        "if train_dirs:\n",
        "    latest_train_dir = train_dirs[0]\n",
        "    print(f\"Found latest training directory: {latest_train_dir}\")\n",
        "\n",
        "    # Check if best model exists (looking for safetensors format)\n",
        "    best_model_path = os.path.join(latest_train_dir, \"best_model/model.safetensors\")\n",
        "    if os.path.exists(best_model_path):\n",
        "        print(\"Training completed successfully! Best model was saved.\")\n",
        "        # List model files\n",
        "        model_files = os.listdir(os.path.join(latest_train_dir, \"best_model\"))\n",
        "        print(f\"Model files: {model_files}\")\n",
        "\n",
        "        # Check if metrics history exists\n",
        "        metrics_path = os.path.join(latest_train_dir, \"metrics_history.json\")\n",
        "        if os.path.exists(metrics_path):\n",
        "            print(\"Training metrics were saved. You can use them for analysis.\")\n",
        "\n",
        "        # Check if log file exists\n",
        "        log_path = os.path.join(latest_train_dir, \"training_log.txt\")\n",
        "        if os.path.exists(log_path):\n",
        "            print(\"\\nLast few lines from training log:\")\n",
        "            # Display the last 10 lines of the log file\n",
        "            with open(log_path, \"r\") as f:\n",
        "                log_lines = f.readlines()\n",
        "                for line in log_lines[-10:]:\n",
        "                    print(line.strip())\n",
        "    else:\n",
        "        print(\"Best model not found in the latest training directory.\")\n",
        "\n",
        "        # Check if final model exists\n",
        "        final_model_path = os.path.join(latest_train_dir, \"final_model/model.safetensors\")\n",
        "        if os.path.exists(final_model_path):\n",
        "            print(\"Final model was saved, but it wasn't the best during training.\")\n",
        "            print(f\"You can find it at: {os.path.join(latest_train_dir, 'final_model')}\")\n",
        "        else:\n",
        "            print(\"Neither best nor final models were found. Training may have failed.\")\n",
        "\n",
        "        # Check if log file exists for debugging\n",
        "        log_path = os.path.join(latest_train_dir, \"training_log.txt\")\n",
        "        if os.path.exists(log_path):\n",
        "            print(\"\\nLast few lines from training log (might help diagnose issues):\")\n",
        "            # Display the last 10 lines of the log file\n",
        "            with open(log_path, \"r\") as f:\n",
        "                log_lines = f.readlines()\n",
        "                for line in log_lines[-10:]:\n",
        "                    print(line.strip())\n",
        "else:\n",
        "    print(\"No training directories found at layoutlmv3/model/train-*\")\n",
        "\n",
        "    # Fallback to check the old directory structure\n",
        "    if os.path.exists(\"layoutlmv3/model/best_model/model.safetensors\") or os.path.exists(\"layoutlmv3/model/best_model/pytorch_model.bin\"):\n",
        "        print(\"Found model in the old directory structure.\")\n",
        "        print(\"Training completed successfully! Best model was saved.\")\n",
        "        # List model files\n",
        "        model_files = os.listdir(\"layoutlmv3/model/best_model\")\n",
        "        print(f\"Model files: {model_files}\")\n",
        "    else:\n",
        "        print(\"No trained models found. Please run training first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C93hTdqj0DFV"
      },
      "source": [
        "### 3.4 Check Training Logs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpQIE4fez-Cg",
        "outputId": "85aa37d3-969a-4bd9-d6b7-1616f69dfe2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Latest training directory: layoutlmv3/model/train-20250404_165409\n",
            "Found training log at: layoutlmv3/model/train-20250404_165409/training_log.txt\n",
            "\n",
            "=== Last 50 lines of training log ===\n",
            "\n",
            "Evaluating after epoch 18...\n",
            "Evaluation results: {'precision': 0.014796468533497116, 'recall': 0.12164073550212164, 'f1': 0.02638361476212475}\n",
            "Epoch 19/25\n",
            "Epoch 19 average loss: 2.9502\n",
            "Evaluating after epoch 19...\n",
            "Evaluation results: {'precision': 0.014796468533497116, 'recall': 0.12164073550212164, 'f1': 0.02638361476212475}\n",
            "Epoch 20/25\n",
            "Step 1000/1300 - Loss: 0.0349\n",
            "\n",
            "Evaluating at step 1000...\n",
            "Evaluation results: {'precision': 0.014796468533497116, 'recall': 0.12164073550212164, 'f1': 0.02638361476212475}\n",
            "Epoch 20 average loss: 2.9417\n",
            "Evaluating after epoch 20...\n",
            "Evaluation results: {'precision': 0.014796468533497116, 'recall': 0.12164073550212164, 'f1': 0.02638361476212475}\n",
            "Epoch 21/25\n",
            "Epoch 21 average loss: 2.9023\n",
            "Evaluating after epoch 21...\n",
            "Evaluation results: {'precision': 0.014796468533497116, 'recall': 0.12164073550212164, 'f1': 0.02638361476212475}\n",
            "Epoch 22/25\n",
            "Step 1100/1300 - Loss: 0.0196\n",
            "\n",
            "Evaluating at step 1100...\n",
            "Evaluation results: {'precision': 0.014796468533497116, 'recall': 0.12164073550212164, 'f1': 0.02638361476212475}\n",
            "Epoch 22 average loss: 2.9551\n",
            "Evaluating after epoch 22...\n",
            "Evaluation results: {'precision': 0.0027388271257919893, 'recall': 0.05233380480905234, 'f1': 0.005205244026706818}\n",
            "Epoch 23/25\n",
            "Epoch 23 average loss: 2.9112\n",
            "Evaluating after epoch 23...\n",
            "Evaluation results: {'precision': 0.014796468533497116, 'recall': 0.12164073550212164, 'f1': 0.02638361476212475}\n",
            "Epoch 24/25\n",
            "Step 1200/1300 - Loss: 0.0088\n",
            "\n",
            "Evaluating at step 1200...\n",
            "Evaluation results: {'precision': 0.0027388271257919893, 'recall': 0.05233380480905234, 'f1': 0.005205244026706818}\n",
            "Epoch 24 average loss: 2.9239\n",
            "Evaluating after epoch 24...\n",
            "Evaluation results: {'precision': 0.0027388271257919893, 'recall': 0.05233380480905234, 'f1': 0.005205244026706818}\n",
            "Epoch 25/25\n",
            "Step 1300/1300 - Loss: 0.1171\n",
            "\n",
            "Evaluating at step 1300...\n",
            "Evaluation results: {'precision': 0.0027388271257919893, 'recall': 0.05233380480905234, 'f1': 0.005205244026706818}\n",
            "Epoch 25 average loss: 2.9270\n",
            "Evaluating after epoch 25...\n",
            "Evaluation results: {'precision': 0.0027388271257919893, 'recall': 0.05233380480905234, 'f1': 0.005205244026706818}\n",
            "Training complete. Saving final model...\n",
            "Best F1: 0.0568\n",
            "Model saved to layoutlmv3/model/train-20250404_165409\n",
            "Training completed at: 2025-04-04 17:08:57\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Find the most recent training directory\n",
        "train_dirs = sorted(glob.glob(\"layoutlmv3/model/train-*\"), reverse=True)\n",
        "\n",
        "if train_dirs:\n",
        "    latest_train_dir = train_dirs[0]\n",
        "    print(f\"Latest training directory: {latest_train_dir}\")\n",
        "\n",
        "    # Path to the training log\n",
        "    log_path = os.path.join(latest_train_dir, \"training_log.txt\")\n",
        "\n",
        "    if os.path.exists(log_path):\n",
        "        print(f\"Found training log at: {log_path}\")\n",
        "\n",
        "        # Option 1: Print the entire log\n",
        "        # with open(log_path, \"r\") as f:\n",
        "        #     print(f.read())\n",
        "\n",
        "        # Option 2: Print the last N lines (useful for long logs)\n",
        "        num_lines = 50  # Adjust this number to see more or fewer lines\n",
        "        with open(log_path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        print(f\"\\n=== Last {min(num_lines, len(lines))} lines of training log ===\\n\")\n",
        "        for line in lines[-num_lines:]:\n",
        "            print(line.strip())\n",
        "    else:\n",
        "        print(f\"No training log found at {log_path}\")\n",
        "else:\n",
        "    print(\"No training directories found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olnAmB_s7ROP"
      },
      "source": [
        "## 4. Inference and Evaluation\n",
        "\n",
        "Now let's use our trained model to analyze new legal documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JklhvFHR7ROP"
      },
      "source": [
        "### 4.1 Upload or Select Test Document\n",
        "\n",
        "First, let's select a PDF document to analyze. You can either use one from the test set or upload a new one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "AE1kQMLS7ROP",
        "outputId": "575847ee-be6b-48ae-c13f-4a2af64f66d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload a PDF document for analysis:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c6dac79c-7cc7-4e0e-918f-399df0d939f1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c6dac79c-7cc7-4e0e-918f-399df0d939f1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving CreditcardscomInc_20070810_S-1_EX-10.33_362297_EX-10.33_Affiliate Agreement.pdf to CreditcardscomInc_20070810_S-1_EX-10.33_362297_EX-10.33_Affiliate Agreement.pdf\n",
            "Selected PDF for analysis: CreditcardscomInc_20070810_S-1_EX-10.33_362297_EX-10.33_Affiliate Agreement.pdf\n"
          ]
        }
      ],
      "source": [
        "# Option 1: Upload a new PDF (for Colab)\n",
        "if IN_COLAB:\n",
        "    from google.colab import files\n",
        "\n",
        "    print(\"Upload a PDF document for analysis:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if uploaded:\n",
        "        test_pdf_path = list(uploaded.keys())[0]\n",
        "        print(f\"Selected PDF for analysis: {test_pdf_path}\")\n",
        "    else:\n",
        "        print(\"No file uploaded. Please select an existing PDF.\")\n",
        "        test_pdf_path = None\n",
        "else:\n",
        "    # Option 2: Use an existing PDF\n",
        "    # Find PDFs in the CUAD dataset\n",
        "    pdf_dir = \"CUAD_v1/full_contract_pdf\"\n",
        "    if os.path.exists(pdf_dir) and os.listdir(pdf_dir):\n",
        "        pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith('.pdf')][:10]  # List first 10 files\n",
        "\n",
        "        if pdf_files:\n",
        "            print(\"Available PDFs for analysis:\")\n",
        "            for i, pdf in enumerate(pdf_files):\n",
        "                print(f\"{i+1}. {pdf}\")\n",
        "\n",
        "            # Select a PDF (you can change the index)\n",
        "            selected_idx = 0  # SEE HERE!!!! Change this to select a different file\n",
        "            test_pdf_path = os.path.join(pdf_dir, pdf_files[selected_idx])\n",
        "            print(f\"\\nSelected PDF for analysis: {test_pdf_path}\")\n",
        "        else:\n",
        "            print(\"No PDF files found in the dataset directory.\")\n",
        "            test_pdf_path = None\n",
        "    else:\n",
        "        print(\"PDF directory is empty or doesn't exist. Please add PDF files for analysis.\")\n",
        "        test_pdf_path = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCLAI8vp7bVQ",
        "outputId": "a9ca7a71-480c-4dcd-ad91-2bf77ea59b80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creator:         Aspose Ltd.\n",
            "Producer:        Aspose.Pdf for .NET 17.6\n",
            "ModDate:         Tue Mar 31 03:29:13 2020 UTC\n",
            "Custom Metadata: no\n",
            "Metadata Stream: yes\n",
            "Tagged:          no\n",
            "UserProperties:  no\n",
            "Suspects:        no\n",
            "Form:            none\n",
            "JavaScript:      no\n",
            "Pages:           12\n",
            "Encrypted:       no\n",
            "Page size:       595 x 842 pts (A4)\n",
            "Page rot:        0\n",
            "File size:       133922 bytes\n",
            "Optimized:       no\n",
            "PDF version:     1.4\n"
          ]
        }
      ],
      "source": [
        "!pdfinfo \"{test_pdf_path}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv0dg1I77ROP"
      },
      "source": [
        "### 4.2 Run Inference\n",
        "\n",
        "Let's analyze the selected document with our trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZyRUaSE7ROP",
        "outputId": "e74a1db7-fbf6-46e5-af12-7bdc0f06ff69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found latest training directory: layoutlmv3/model/train-20250404_165409\n",
            "Running inference using model: layoutlmv3/model/train-20250404_165409/best_model\n",
            "Using processor from: layoutlmv3/model/train-20250404_165409\n",
            "2025-04-04 18:22:50.303288: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743790970.619104   54922 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743790970.701033   54922 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-04 18:22:51.337632: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
            "  warnings.warn(warning_message)\n",
            "Loading model from layoutlmv3/model/train-20250404_165409/best_model\n",
            "Loaded label map with 41 categories: ['Document Name', 'Parties', 'Agreement Date', 'Effective Date', 'Expiration Date', 'Renewal Term', 'Notice Period To Terminate Renewal', 'Governing Law', 'Most Favored Nation', 'Non-Compete', 'Exclusivity', 'No-Solicit Of Customers', 'No-Solicit Of Employees', 'Non-Disparagement', 'Termination For Convenience', 'Rofr/Rofo/Rofn', 'Change Of Control', 'Anti-Assignment', 'Revenue/Profit Sharing', 'Price Restrictions', 'Minimum Commitment', 'Volume Restriction', 'IP Ownership Assignment', 'Joint IP Ownership', 'License Grant', 'Non-Transferable License', 'Affiliate License-Licensee', 'Affiliate License-Licensor', 'Unlimited/All-You-Can-Eat-License', 'Irrevocable Or Perpetual License', 'Source Code Escrow', 'Post-Termination Services', 'Audit Rights', 'Uncapped Liability', 'Cap On Liability', 'Liquidated Damages', 'Warranty Duration', 'Insurance', 'Covenant Not To Sue', 'Third Party Beneficiary', 'O']\n",
            "Model loaded and moved to cuda\n",
            "Processing PDF: CreditcardscomInc_20070810_S-1_EX-10.33_362297_EX-10.33_Affiliate Agreement.pdf (Using internal path: CreditcardscomInc_20070810_S-1_EX-10.33_362297_EX-10.33_Affiliate Agreement.pdf)\n",
            "Converting PDF to images using DPI=300\n",
            "Successfully converted PDF to 12 pages\n",
            "Processing pages:   0% 0/12 [00:00<?, ?it/s]Processing page 1/12\n",
            "Page 1: Got 27 words from OCR\n",
            "Extracting clauses from 27 words with threshold 0.1\n",
            "Available label IDs: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
            "Predictions: 0 words with predictions above threshold\n",
            "Unique prediction IDs: set()\n",
            "Extracted 0 clauses across 0 categories\n",
            "Visualizing page 1...\n",
            "Saved annotated image to layoutlmv3/results/CreditcardscomInc_20070810_S-1_EX-10.33_362297_EX-10.33_Affiliate Agreement/page_1_annotated.jpg\n",
            "Processing pages:   8% 1/12 [00:17<03:17, 17.94s/it]Processing page 2/12\n",
            "Page 2: Got 46 words from OCR\n",
            "Extracting clauses from 46 words with threshold 0.1\n",
            "Available label IDs: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
            "Predictions: 0 words with predictions above threshold\n",
            "Unique prediction IDs: set()\n",
            "Extracted 0 clauses across 0 categories\n",
            "Visualizing page 2...\n",
            "Saved annotated image to layoutlmv3/results/CreditcardscomInc_20070810_S-1_EX-10.33_362297_EX-10.33_Affiliate Agreement/page_2_annotated.jpg\n",
            "Processing pages:  17% 2/12 [00:25<01:58, 11.87s/it]Processing page 3/12\n",
            "Page 3: Got 43 words from OCR\n",
            "Extracting clauses from 43 words with threshold 0.1\n",
            "Available label IDs: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
            "Predictions: 0 words with predictions above threshold\n",
            "Unique prediction IDs: set()\n",
            "Extracted 0 clauses across 0 categories\n",
            "Visualizing page 3...\n",
            "Saved annotated image to layoutlmv3/results/CreditcardscomInc_20070810_S-1_EX-10.33_362297_EX-10.33_Affiliate Agreement/page_3_annotated.jpg\n",
            "Processing pages:  25% 3/12 [00:33<01:32, 10.23s/it]Processing page 4/12\n",
            "^C\n",
            "\n",
            "Results saved to layoutlmv3/results/CreditcardscomInc_20070810_S-1_EX-10.33_362297_EX-10.33_Affiliate Agreement\n",
            "Generated files: ['page_1.jpg', 'page_1_annotated.jpg', 'page_2.jpg', 'page_2_annotated.jpg', 'page_3.jpg', 'page_3_annotated.jpg']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Find the most recent training directory\n",
        "train_dirs = sorted(glob.glob(\"layoutlmv3/model/train-*\"), reverse=True)\n",
        "\n",
        "if train_dirs:\n",
        "    latest_train_dir = train_dirs[0]\n",
        "    print(f\"Found latest training directory: {latest_train_dir}\")\n",
        "\n",
        "    # Check if best model exists (looking for safetensors format)\n",
        "    best_model_path = os.path.join(latest_train_dir, \"best_model\")\n",
        "    model_exists = os.path.exists(os.path.join(best_model_path, \"model.safetensors\"))\n",
        "\n",
        "    if not model_exists:\n",
        "        # Check for final model if best model doesn't exist\n",
        "        final_model_path = os.path.join(latest_train_dir, \"final_model\")\n",
        "        if os.path.exists(os.path.join(final_model_path, \"model.safetensors\")):\n",
        "            model_exists = True\n",
        "            best_model_path = final_model_path\n",
        "            print(\"Best model not found, using final model instead.\")\n",
        "else:\n",
        "    # Fallback to check the old directory structure\n",
        "    best_model_path = \"layoutlmv3/model/best_model\"\n",
        "    model_exists = os.path.exists(os.path.join(best_model_path, \"pytorch_model.bin\")) or os.path.exists(os.path.join(best_model_path, \"model.safetensors\"))\n",
        "\n",
        "if not model_exists:\n",
        "    print(\"Warning: Trained model not found. You need to train the model first.\")\n",
        "    print(\"Check if any other model checkpoints exist that you could use instead:\")\n",
        "\n",
        "    # Look for models in timestamp directories\n",
        "    all_models = []\n",
        "    for train_dir in train_dirs:\n",
        "        for model_subdir in [\"best_model\", \"final_model\"]:\n",
        "            model_dir = os.path.join(train_dir, model_subdir)\n",
        "            if os.path.exists(os.path.join(model_dir, \"model.safetensors\")) or os.path.exists(os.path.join(model_dir, \"pytorch_model.bin\")):\n",
        "                all_models.append(model_dir)\n",
        "\n",
        "    # Also check old directory structure\n",
        "    for d in os.listdir(\"layoutlmv3/model\"):\n",
        "        model_dir = os.path.join(\"layoutlmv3/model\", d)\n",
        "        if os.path.isdir(model_dir) and (\n",
        "            os.path.exists(os.path.join(model_dir, \"pytorch_model.bin\")) or\n",
        "            os.path.exists(os.path.join(model_dir, \"model.safetensors\"))\n",
        "        ):\n",
        "            all_models.append(model_dir)\n",
        "\n",
        "    if all_models:\n",
        "        print(f\"Found {len(all_models)} models: {all_models}\")\n",
        "        print(\"You could use one of these for inference by modifying the --model_dir parameter\")\n",
        "    else:\n",
        "        print(\"No models found. Please train the model first.\")\n",
        "\n",
        "# Find processor directory\n",
        "if train_dirs:\n",
        "    processor_dir = latest_train_dir\n",
        "else:\n",
        "    processor_dir = \"layoutlmv3/model\"\n",
        "\n",
        "# Run inference on the selected PDF if a model exists and a PDF is selected\n",
        "if model_exists and test_pdf_path:\n",
        "    print(f\"Running inference using model: {best_model_path}\")\n",
        "    print(f\"Using processor from: {processor_dir}\")\n",
        "\n",
        "    !python layoutlmv3/inference.py \\\n",
        "        --model_dir \"{best_model_path}\" \\\n",
        "        --processor_dir \"{processor_dir}\" \\\n",
        "        --pdf_path \"{test_pdf_path}\" \\\n",
        "        --output_dir layoutlmv3/results \\\n",
        "        --device {device} \\\n",
        "        --confidence_threshold 0.1\n",
        "\n",
        "    # Check results\n",
        "    pdf_name = os.path.splitext(os.path.basename(test_pdf_path))[0]\n",
        "    # Sanitize the filename the same way as in inference.py\n",
        "    safe_name = \"\".join(c for c in pdf_name if c.isalnum() or c in \"._- \")\n",
        "    results_dir = os.path.join(\"layoutlmv3/results\", safe_name)\n",
        "\n",
        "    if os.path.exists(results_dir):\n",
        "        print(f\"\\nResults saved to {results_dir}\")\n",
        "        # List results files\n",
        "        result_files = os.listdir(results_dir)\n",
        "        print(f\"Generated files: {result_files}\")\n",
        "\n",
        "        # Display summary if available\n",
        "        summary_path = os.path.join(results_dir, \"summary.txt\")\n",
        "        if os.path.exists(summary_path):\n",
        "            print(\"\\n=== ANALYSIS SUMMARY ===\\n\")\n",
        "            with open(summary_path, \"r\") as f:\n",
        "                summary = f.read()\n",
        "                # Print the first 1000 characters of the summary\n",
        "                print(summary[:1000] + \"...\" if len(summary) > 1000 else summary)\n",
        "    else:\n",
        "        print(f\"Results directory not found: {results_dir}\")\n",
        "        # Try to find any directories that might have been created\n",
        "        possible_dirs = glob.glob(f\"layoutlmv3/results/*{pdf_name.split('_')[0]}*\")\n",
        "        if possible_dirs:\n",
        "            print(f\"Found possible result directories: {possible_dirs}\")\n",
        "elif test_pdf_path is None:\n",
        "    print(\"No PDF selected for analysis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20_gXwP97ROQ"
      },
      "source": [
        "### 4.3 Display Results\n",
        "\n",
        "If we're in a notebook environment, let's display some visualization of the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHYMy-z77ROQ"
      },
      "outputs": [],
      "source": [
        "# Display image results if available\n",
        "if 'results_dir' in locals() and os.path.exists(results_dir):\n",
        "    import matplotlib.pyplot as plt\n",
        "    from PIL import Image\n",
        "\n",
        "    # Find annotated images\n",
        "    try:\n",
        "        image_files = [f for f in os.listdir(results_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "        annotated_images = [f for f in image_files if 'annotated' in f]\n",
        "\n",
        "        if annotated_images:\n",
        "            print(f\"Found {len(annotated_images)} annotated images\")\n",
        "\n",
        "            # Display the first few annotated images (limited to 3 for space)\n",
        "            display_images = annotated_images[:3]\n",
        "            plt.figure(figsize=(15, 5 * len(display_images)))\n",
        "\n",
        "            for i, img_file in enumerate(display_images):\n",
        "                img_path = os.path.join(results_dir, img_file)\n",
        "                img = Image.open(img_path)\n",
        "\n",
        "                plt.subplot(len(display_images), 1, i+1)\n",
        "                plt.imshow(img)\n",
        "                plt.title(f\"Page {i+1} Analysis Results\")\n",
        "                plt.axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"No annotated images found in the results directory.\")\n",
        "\n",
        "            # Check for regular (non-annotated) images as fallback\n",
        "            regular_images = image_files[:3]  # Just show first 3\n",
        "            if regular_images:\n",
        "                print(f\"Found {len(image_files)} regular images\")\n",
        "                plt.figure(figsize=(15, 5 * len(regular_images)))\n",
        "\n",
        "                for i, img_file in enumerate(regular_images):\n",
        "                    img_path = os.path.join(results_dir, img_file)\n",
        "                    img = Image.open(img_path)\n",
        "\n",
        "                    plt.subplot(len(regular_images), 1, i+1)\n",
        "                    plt.imshow(img)\n",
        "                    plt.title(f\"Page {i+1}\")\n",
        "                    plt.axis('off')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "        # Load and display extracted clauses\n",
        "        clauses_path = os.path.join(results_dir, \"extracted_clauses.json\")\n",
        "        if os.path.exists(clauses_path):\n",
        "            import json\n",
        "            with open(clauses_path, \"r\") as f:\n",
        "                clauses = json.load(f)\n",
        "\n",
        "            print(\"\\n=== EXTRACTED CLAUSES ===\\n\")\n",
        "            for category, category_clauses in clauses.items():\n",
        "                print(f\"\\n## {category.upper()} ({len(category_clauses)} clauses)\")\n",
        "                for i, clause in enumerate(category_clauses[:2]):  # Show first 2 clauses per category\n",
        "                    if isinstance(clause, dict):\n",
        "                        text = clause.get(\"text\", \"\")\n",
        "                        confidence = clause.get(\"confidence\", 0.0)\n",
        "                        page = clause.get(\"page\", \"?\")\n",
        "                        print(f\"\\n{i+1}. [Page {page}, Confidence: {confidence:.2f}] {text[:200]}...\" if len(text) > 200\n",
        "                              else f\"\\n{i+1}. [Page {page}, Confidence: {confidence:.2f}] {text}\")\n",
        "                    else:\n",
        "                        print(f\"\\n{i+1}. {clause[:200]}...\" if len(clause) > 200 else f\"\\n{i+1}. {clause}\")\n",
        "                if len(category_clauses) > 2:\n",
        "                    print(f\"\\n...and {len(category_clauses)-2} more clauses\")\n",
        "        else:\n",
        "            print(\"No extracted clauses file found in the results directory.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error displaying results: {e}\")\n",
        "else:\n",
        "    print(\"No results directory available. Run inference first to generate results.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu5TDkKO7ROQ"
      },
      "source": [
        "## 5. Conclusion and Next Steps\n",
        "\n",
        "You have now set up the complete pipeline for using LayoutLMv3 to analyze legal documents!\n",
        "\n",
        "### Summary of the Process:\n",
        "\n",
        "1. **Environment Setup**: Configured the environment and installed dependencies\n",
        "2. **Data Preparation**: Used prepared dataset or created it from CUAD annotations\n",
        "3. **Model Training**: Fine-tuned LayoutLMv3 for legal document understanding\n",
        "4. **Inference**: Applied the trained model to analyze new documents\n",
        "\n",
        "### Potential Next Steps:\n",
        "\n",
        "1. **Optimize model performance**: Experiment with different training parameters, data augmentation, or model architectures\n",
        "2. **Expand to more clause types**: Train the model on additional clause categories\n",
        "3. **Integrate with a user interface**: Create a web application for document analysis\n",
        "4. **Post-processing improvements**: Develop better methods to extract and summarize clauses\n",
        "5. **Cross-validation**: Implement k-fold cross-validation to ensure model robustness\n",
        "\n",
        "This project demonstrates how pre-trained document understanding models like LayoutLMv3 can be leveraged for specialized domains like legal document analysis. The same approach can be adapted to other document-intensive domains such as financial documents, medical records, or technical specifications."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
